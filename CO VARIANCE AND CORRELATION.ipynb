{
    "nbformat_minor": 1, 
    "cells": [
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20190516121619-0000\nKERNEL_ID = c3e93e83-6787-44c3-8cde-9b0264a4b8a4\n"
                }
            ], 
            "source": "rddX = sc.parallelize(range(100))\nrddY = sc.parallelize(range(100))"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "49.5\n49.5\n"
                }
            ], 
            "source": "meanX = rddX.sum()/rddX.count()\nmeanY = rddY.sum()/rddY.count()\nprint(meanX)\nprint(meanY)"
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 3, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[(0, 0),\n (1, 1),\n (2, 2),\n (3, 3),\n (4, 4),\n (5, 5),\n (6, 6),\n (7, 7),\n (8, 8),\n (9, 9)]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "rddXY = rddX.zip(rddY)\nrddXY.take(10)"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "833.25\n"
                }
            ], 
            "source": "#According to PEP3113 the Python Tuple Parameter unpacking have been discontinued. The folllowing is the new approach. \ncovXY = rddXY.map(lambda xy: (xy[0]-meanX)*(xy[1]-meanY)).sum()/rddXY.count()\nprint(covXY)"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "28.86607004772212\n28.86607004772212\n"
                }
            ], 
            "source": "from math import sqrt\nn = rddXY.count()\nsdX = sqrt(rddX.map(lambda x: pow(x - meanX, 2)).sum()/n)\nsdY = sqrt(rddY.map(lambda y: pow(y - meanY, 2)).sum()/n)\nprint(sdX)\nprint(sdY)"
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 13, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "1.0"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "corrXY = covXY/(sdX*sdY)\ncorrXY"
        }, 
        {
            "execution_count": 53, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "column1 = sc.parallelize(range(100))"
        }, 
        {
            "execution_count": 54, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "column2 = sc.parallelize(range(100, 200))\ncolumn3 = sc.parallelize(list(reversed(range(100))))\nimport random\ncolumn4 = sc.parallelize(random.sample(range(100), 100))"
        }, 
        {
            "execution_count": 55, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from pyspark.mllib.stat import Statistics"
        }, 
        {
            "execution_count": 88, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 88, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[(((0, 100), 99), 4),\n (((1, 101), 98), 11),\n (((2, 102), 97), 26),\n (((3, 103), 96), 47),\n (((4, 104), 95), 87),\n (((5, 105), 94), 53),\n (((6, 106), 93), 89),\n (((7, 107), 92), 15),\n (((8, 108), 91), 31),\n (((9, 109), 90), 40),\n (((10, 110), 89), 55),\n (((11, 111), 88), 36),\n (((12, 112), 87), 48),\n (((13, 113), 86), 70),\n (((14, 114), 85), 65),\n (((15, 115), 84), 12),\n (((16, 116), 83), 62),\n (((17, 117), 82), 41),\n (((18, 118), 81), 29),\n (((19, 119), 80), 35),\n (((20, 120), 79), 10),\n (((21, 121), 78), 86),\n (((22, 122), 77), 99),\n (((23, 123), 76), 38),\n (((24, 124), 75), 83),\n (((25, 125), 74), 3),\n (((26, 126), 73), 59),\n (((27, 127), 72), 68),\n (((28, 128), 71), 21),\n (((29, 129), 70), 64),\n (((30, 130), 69), 5),\n (((31, 131), 68), 94),\n (((32, 132), 67), 81),\n (((33, 133), 66), 42),\n (((34, 134), 65), 58),\n (((35, 135), 64), 95),\n (((36, 136), 63), 61),\n (((37, 137), 62), 66),\n (((38, 138), 61), 96),\n (((39, 139), 60), 37),\n (((40, 140), 59), 98),\n (((41, 141), 58), 82),\n (((42, 142), 57), 33),\n (((43, 143), 56), 63),\n (((44, 144), 55), 24),\n (((45, 145), 54), 67),\n (((46, 146), 53), 76),\n (((47, 147), 52), 54),\n (((48, 148), 51), 60),\n (((49, 149), 50), 6),\n (((50, 150), 49), 46),\n (((51, 151), 48), 16),\n (((52, 152), 47), 73),\n (((53, 153), 46), 88),\n (((54, 154), 45), 1),\n (((55, 155), 44), 72),\n (((56, 156), 43), 57),\n (((57, 157), 42), 43),\n (((58, 158), 41), 2),\n (((59, 159), 40), 7),\n (((60, 160), 39), 30),\n (((61, 161), 38), 90),\n (((62, 162), 37), 32),\n (((63, 163), 36), 45),\n (((64, 164), 35), 80),\n (((65, 165), 34), 44),\n (((66, 166), 33), 85),\n (((67, 167), 32), 49),\n (((68, 168), 31), 51),\n (((69, 169), 30), 23),\n (((70, 170), 29), 75),\n (((71, 171), 28), 69),\n (((72, 172), 27), 25),\n (((73, 173), 26), 20),\n (((74, 174), 25), 91),\n (((75, 175), 24), 18),\n (((76, 176), 23), 74),\n (((77, 177), 22), 92),\n (((78, 178), 21), 79),\n (((79, 179), 20), 39),\n (((80, 180), 19), 34),\n (((81, 181), 18), 17),\n (((82, 182), 17), 93),\n (((83, 183), 16), 9),\n (((84, 184), 15), 0),\n (((85, 185), 14), 50),\n (((86, 186), 13), 71),\n (((87, 187), 12), 77),\n (((88, 188), 11), 14),\n (((89, 189), 10), 84),\n (((90, 190), 9), 22),\n (((91, 191), 8), 78),\n (((92, 192), 7), 56),\n (((93, 193), 6), 52),\n (((94, 194), 5), 27),\n (((95, 195), 4), 19),\n (((96, 196), 3), 13),\n (((97, 197), 2), 28),\n (((98, 198), 1), 97),\n (((99, 199), 0), 8)]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "data = column1.zip(column2).zip(column3).zip(column4)\ndata.take(100)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import re\nre.findall(r\"\\d{1,})"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5 with Spark", 
            "name": "python3", 
            "language": "python3"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.4", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}